# 5.1 Evaluating generative text models
import torch
import tiktoken

from chapter4.gpt_model import GPTModel
from chapter4.util import generate_text_simple
from .util import text_to_token_ids, token_ids_to_text, calc_loss_batch

"""
we will set up our model for text generation and look at basic ways to evaluate the quality of the generated text, and calculate the training and validation losses
"""

GPT_CONFIG_124M = {
    "vocab_size": 50257,  # Vocabulary size
    "context_length": 256,  # Context length
    "emb_dim": 768,  # Embedding dimension
    "n_heads": 12,  # Number of attention heads
    "n_layers": 12,  # Number of layers
    "drop_rate": 0.1,  # Dropout rate
    "qkv_bias": False,  # Query-Key-Value bias
}
torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.eval()

start_content = "Every effort moves you"
tokenizer = tiktoken.get_encoding("gpt2")

token_ids = generate_text_simple(
    model=model,
    idx=text_to_token_ids(start_content, tokenizer),
    max_new_tokens=10,
    context_size=GPT_CONFIG_124M["context_length"],
)
print(f"Output text: {token_ids_to_text(token_ids, tokenizer)}")

"""
look at how to assess text quality generated by a generative model by calculating a text generation loss

steps:
1. use vocabulary to map text to token IDs
2. get high dimensional probablility row vector for each token with softmax
3. locate the index position of the token ID in the row vector with the highest probability using argmax
4. get all predicted token IDs as the indices of the highest probability token IDs
5. map the predicted token IDs back to text

"""
# "Every effort moves"
# "I really like"
inputs = torch.tensor([[16833, 3626, 6100], [40, 1107, 588]])

# targets are one token later than the inputs, this is so we can teach the model to predict the next token
# "effort moves you"
# "really like chocolate"
targets = torch.tensor([[3626, 6100, 345], [1107, 588, 11311]])

with torch.no_grad():
    logits = model(inputs)
probas = torch.softmax(logits, dim=-1)
print(probas.shape)
# returns (2, 3, 50257), which means 2 batches, 3 tokens, and 50257 vocab size

token_ids = torch.argmax(probas, dim=-1, keepdim=True)
print(f"Token IDs: {token_ids}")

print(f"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}")
print(f"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}")

# targets does not match the output because it's not been trained yet
# we can now evaluate the performance of the model's generated text with a loss function, so we can measure the quality of the generated text and also use it to implement the training function

text_idx = 0
# select the first text, then all 3 tokens, and then the target token IDs
target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]
print(f"Text 1: {target_probas_1}")

text_idx = 1
target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]
print(f"Text 2: {target_probas_2}")

log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))
print(f"Log probability: {log_probas}")

avg_log_probas = torch.mean(log_probas)
print(f"Avg log probability: {avg_log_probas}")

neg_avg_log_probas = avg_log_probas * -1
print(f"Negative avg log probability: {neg_avg_log_probas}")

# calculating loss involves following steps
# 1. get logits
# 2. get the probabilities with softmax
# 3. get the target probabilities
# 4. get the log probabilities
# 5. get the mean of the log probabilities
# 6. get the negative of the mean log probabilities

# these steps are already combined in the cross_entropy loss function, so we can use that

print(f"Logits shape: {logits.shape}")
print(f"Targets shape: {targets.shape}")
logits_flat = logits.flatten(0, 1)
targets_flat = targets.flatten()
print(f"Flattened logits shape: {logits_flat.shape}")
print(f"Flattened targets shape: {targets_flat.shape}")

loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)
print(f"Cross entropy loss: {loss}")

# perplexity is a measure of how well a probability distribution predicts a sample
# often used to evaluate language models as well
# it is the exponentiation of the cross entropy loss

perplexity = torch.exp(loss)
print(f"Perplexity: {perplexity}")

# 5.1.3 calculating training and validation set losses
# here we will divide the dataset into training and validation sets, and calculate the training and validation losses
import os

current_dir = os.path.dirname(os.path.abspath(__file__))
file_path = os.path.join(current_dir, "..", "the-verdict.txt")
with open(file_path, "r", encoding="utf-8") as f:
    text_data = f.read()

total_characters = len(text_data)
total_tokens = len(tokenizer.encode(text_data))
print(f"Total characters: {total_characters}")
print(f"Total tokens: {total_tokens}")

train_ratio = 0.90
split_idx = int(train_ratio * len(text_data))
train_data = text_data[:split_idx]
val_data = text_data[split_idx:]

from chapter2.gpt_dataset import create_dataloader_v1

torch.manual_seed(123)

train_loader = create_dataloader_v1(
    train_data,
    batch_size=2,
    max_length=GPT_CONFIG_124M["context_length"],
    stride=GPT_CONFIG_124M["context_length"],
    drop_last=True,
    shuffle=True,
    num_workers=0,
)

val_loader = create_dataloader_v1(
    val_data,
    batch_size=2,
    max_length=GPT_CONFIG_124M["context_length"],
    stride=GPT_CONFIG_124M["context_length"],
    drop_last=False,
    shuffle=False,
    num_workers=0,
)

print("Train loader")
for x, y in train_loader:
    print(x.shape, y.shape)

print("Validation loader")
for x, y in val_loader:
    print(x.shape, y.shape)

from .util import calc_loss_loader

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device.type)
model.to(device)
with torch.no_grad():
    train_loss = calc_loss_loader(train_loader, model, device)
    val_loss = calc_loss_loader(val_loader, model, device)
print(f"Train loss: {train_loss}")
print(f"Validation loss: {val_loss}")

# 5.2 finally training

"""
steps of loop:
1. iterate over training epochs (1 epoch is one complete pass over a training set)
2. iterate over batches in each training epoch
3. reset loss gradients from previous batch iteration
4. calculate loss on current batch
5. backward pass to calculate loss gradients
6. update model weights using loss gradients
7. print training and validation set losses
8. generate sample text for visual inspection
"""

def train_model_simple(model, train_loader, val_loader, optimizer,
                       device, num_epochs, eval_freq, eval_iter,
                       start_context, tokenizer):
    train_losses, val_losses, track_tokens_seen = [], [], []
    tokens_seen, global_step = 0, -1
    for epoch in range(num_epochs):
        model.train()
        for input_batch, target_batch in train_loader:
            optimizer.zero_grad()
            loss = calc_loss_batch(
                input_batch, target_batch, model, device
            )
            loss.backward()
            optimizer.step()
            tokens_seen += input_batch.numel()
            global_step += 1

            if global_step % eval_freq == 0:
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader, device, eval_iter
                )
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                track_tokens_seen.append(tokens_seen)
                print(f"Ep {epoch + 1} (Step {global_step:06d}): "
                      f"Train loss {train_loss:.3f}"
                      f"Val loss {val_loss:.3f}")

        generate_and_print_sample(
            model, tokenizer, device, start_context
        )
    return train_losses, val_losses, track_tokens_seen

def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    model.eval()
    with torch.no_grad():
        train_loss = calc_loss_loader(
            train_loader, model, device, num_batches=eval_iter
        )
        val_loss = calc_loss_loader(
            val_loader, model, device, num_batches=eval_iter
        )
    model.train()
    return train_loss, val_loss

def generate_and_print_sample(model, tokenizer, device, start_context):
    model.eval()
    context_size = model.pos_emb.weight.shape[0]
    encoded = text_to_token_ids(start_context, tokenizer).to(device)
    with torch.no_grad():
        token_ids = generate_text_simple(
            model=model, idx=encoded,
            max_new_tokens=50, context_size=context_size
        )
    decoded_text = token_ids_to_text(token_ids, tokenizer)
    print(decoded_text.replace("\n", " "))
    model.train()

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.to(device)
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=0.0004, weight_decay=0.1
)
num_epochs = 10
train_losses, val_losses, tokens_seen = train_model_simple(
    model, train_loader, val_loader, optimizer, device,
    num_epochs, eval_freq=5, eval_iter=5,
    start_context="Every effort moves you", tokenizer=tokenizer
)

import matplotlib
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator

def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):
    fig, ax1 = plt.subplots(figsize=(5, 3))
    ax1.plot(epochs_seen, train_losses, label="Training loss")
    ax1.plot(
        epochs_seen, val_losses, linestyle="-.", label="Validation loss"
    )
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel("Loss")
    ax1.legend(loc="upper right")
    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))
    ax2 = ax1.twiny()
    ax2.plot(tokens_seen, train_losses, alpha=0)
    ax2.set_xlabel("Tokens seen")
    fig.tight_layout()
    plt.show()
    # plt.savefig("output.png", dpi=300, bbox_inches="tight")
    # plt.close()

epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)

"""
here, training and validation loss diverge because the model is overfitting to the training data
this is expected because the training dataset is very small, and we are doing multiple epochs
usually, we do a much larger dataset for only one epoch
"""

model.to("cpu")
model.eval()

tokenizer = tiktoken.get_encoding("gpt2")
token_ids = generate_text_simple(
    model=model,
    idx=text_to_token_ids("Every effort moves you", tokenizer),
    max_new_tokens=25,
    context_size=GPT_CONFIG_124M["context_length"]
)
print(f"Output text:\n{token_ids_to_text(token_ids, tokenizer)}")

# 5.3.1 temperature scaling

vocab = {
    "closer": 0,
    "every": 1,
    "effort": 2,
    "forward": 3,
    "inches": 4,
    "moves": 5,
    "pizza": 6,
    "toward": 7,
    "you": 8,
}
inverse_vocab = {v: k for k, v in vocab.items()}

next_token_logits = torch.tensor(
    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]
)

probas = torch.softmax(next_token_logits, dim=0)
next_token_id = torch.argmax(probas).item()
print(inverse_vocab[next_token_id])

torch.manual_seed(123)
next_token_id = torch.multinomial(probas, num_samples=1).item()
print(inverse_vocab[next_token_id])

def print_sampled_tokens(probas):
    torch.manual_seed(123)
    sample = [torch.multinomial(probas, num_samples=1).item()
             for i in range(1_000)]
    sampled_ids = torch.bincount(torch.tensor(sample))
    for i, freq in enumerate(sampled_ids):
        print(f"{freq} x {inverse_vocab[i]}")

print_sampled_tokens(probas)

def softmax_with_temperature(logits, temperature):
    scaled_logits = logits / temperature
    return torch.softmax(scaled_logits, dim=0)

temperatures = [1, 0.1, 5]
scaled_probas = [softmax_with_temperature(next_token_logits, T)
                for T in temperatures]
x = torch.arange(len(vocab))
bar_width = 0.15
fig, ax = plt.subplots(figsize=(5, 3))
for i, T in enumerate(temperatures):
    rects = ax.bar(x + i * bar_width, scaled_probas[i],
                   bar_width, label=f'Temperature = {T}')
ax.set_ylabel('Probability')
ax.set_xticks(x)
ax.set_xticklabels(vocab.keys(), rotation=90)
ax.legend()
plt.tight_layout()
plt.show()
